{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\user\\\\Desktop\\\\DTI\\\\DeepPurpose-1'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.chdir('../')\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from DeepPurpose import utils, dataset\n",
    "from DeepPurpose import DTI as models\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning Processing...\n",
      "There are 1592671 drug target pairs.\n",
      "Default set to logspace (nM -> p) for easier regression\n"
     ]
    }
   ],
   "source": [
    "#### process_BindingDB ####\n",
    "\n",
    "# Parameter\n",
    "path = '../Database/BindingDB/BindingDB_All_202310_edit.tsv'\n",
    "df = None\n",
    "y = 'IC50'\n",
    "binary = False\n",
    "convert_to_log = True\n",
    "threshold = 30\n",
    "return_ids = False\n",
    "ids_condition = 'OR'\n",
    "harmonize_affinities = None\n",
    "\n",
    "# loading df (bindingDB)\n",
    "df = pd.read_csv(path, sep = '\\t')\n",
    "\n",
    "print('Beginning Processing...')\n",
    "df = df[df['Number of Protein Chains in Target (>1 implies a multichain complex)'] == 1.0]\n",
    "df = df[df['Ligand SMILES'].notnull()]\n",
    "\n",
    "idx_str = []\n",
    "yy = y\n",
    "if isinstance(y, str):\n",
    "\tyy = [y]\n",
    "for y in yy:\n",
    "\tif y == 'Kd':\n",
    "\t\tidx_str.append('Kd (nM)')\n",
    "\telif y == 'IC50':\n",
    "\t\tidx_str.append('IC50 (nM)')\n",
    "\telif y == 'Ki':\n",
    "\t\tidx_str.append('Ki (nM)')\n",
    "\telif y == 'EC50':\n",
    "\t\tidx_str.append('EC50 (nM)')\n",
    "\telse:\n",
    "\t\tprint('select Kd, Ki, IC50 or EC50')\n",
    "\n",
    "if len(idx_str)==1:\n",
    "\tdf_want = df[df[idx_str[0]].notnull()]\n",
    "else: # select multiple affinity measurements.                 \n",
    "\t# keep rows for which at least one of the columns in the idx_str list is not null\n",
    "\tdf_want = df.dropna(thresh=1, subset=idx_str) \n",
    "\t\n",
    "df_want = df_want[['BindingDB Reactant_set_id', 'Ligand InChI', 'Ligand SMILES',\\\n",
    "\t\t\t\t'PubChem CID', 'UniProt (SwissProt) Primary ID of Target Chain',\\\n",
    "\t\t\t\t'BindingDB Target Chain Sequence'] + idx_str]\n",
    "\n",
    "for y in idx_str:\n",
    "\tdf_want[y] = df_want[y].str.replace('>', '')\n",
    "\tdf_want[y] = df_want[y].str.replace('<', '')\n",
    "\tdf_want[y] = df_want[y].astype(float)\n",
    "\n",
    "# Harmonize into single label using the mean of existing labels:\n",
    "df_want['Label'] = df_want[idx_str].mean(axis=1, skipna=True)\n",
    "\n",
    "df_want.rename(columns={'BindingDB Reactant_set_id':'ID',\n",
    "\t\t\t\t\t\t'Ligand SMILES':'SMILES',\n",
    "\t\t\t\t\t\t'Ligand InChI':'InChI',\n",
    "\t\t\t\t\t\t'PubChem CID':'PubChem_ID',\n",
    "\t\t\t\t\t\t'UniProt (SwissProt) Primary ID of Target Chain':'UniProt_ID',\n",
    "\t\t\t\t\t\t'BindingDB Target Chain Sequence': 'Target Sequence'},\n",
    "\t\t\t\t\t\tinplace=True)\n",
    "\n",
    "# have at least uniprot or pubchem ID\n",
    "if ids_condition == 'OR':\n",
    "\tdf_want = df_want[df_want.PubChem_ID.notnull() | df_want.UniProt_ID.notnull()]\n",
    "elif ids_condition == 'AND':\n",
    "\tdf_want = df_want[df_want.PubChem_ID.notnull() & df_want.UniProt_ID.notnull()]\n",
    "else:\n",
    "\tValueError(\"ids_condition must be set to 'OR' or 'AND'\")\n",
    "\n",
    "df_want = df_want[df_want.InChI.notnull()]\n",
    "\n",
    "df_want = df_want[df_want.Label <= 10000000.0]\n",
    "print('There are ' + str(len(df_want)) + ' drug target pairs.')\n",
    "\n",
    "if harmonize_affinities is not None:\n",
    "\tdf_want = df_want[['PubChem_ID', 'SMILES', 'UniProt_ID', 'Target Sequence', 'Label']]\n",
    "\tif harmonize_affinities.lower() == 'max_affinity':\n",
    "\t\tdf_want = df_want.groupby(['PubChem_ID', 'SMILES', 'UniProt_ID', 'Target Sequence']).Label.agg(min).reset_index()\n",
    "\tif harmonize_affinities.lower() == 'mean':\n",
    "\t\tdf_want = df_want.groupby(['PubChem_ID', 'SMILES', 'UniProt_ID', 'Target Sequence']).Label.agg(np.mean).reset_index()\n",
    "\n",
    "if binary:\n",
    "\tprint('Default binary threshold for the binding affinity scores are 30, you can adjust it by using the \"threshold\" parameter')\n",
    "\tif isinstance(threshold, Sequence):\n",
    "\t\t# filter samples with affinity values between the thresholds\n",
    "\t\tdf_want = df_want[(df_want.Label < threshold[0]) | (df_want.Label > threshold[1])]\n",
    "\telse: # single threshold\n",
    "\t\tthreshold = [threshold]\n",
    "\ty = [1 if i else 0 for i in df_want.Label.values < threshold[0]]\n",
    "else:\n",
    "\tif convert_to_log:\n",
    "\t\tprint('Default set to logspace (nM -> p) for easier regression')\n",
    "\t\ty = utils.convert_y_unit(df_want.Label.values, 'nM', 'p')\n",
    "\telse:\n",
    "\t\ty = df_want.Label.values\n",
    "\t\n",
    "if return_ids:\n",
    "\tX_drug, X_target, y, ID_drug, ID_target = df_want.SMILES.values, df_want['Target Sequence'].values, np.array(y), df_want['PubChem_ID'].values, df_want['UniProt_ID'].values\n",
    "X_drug, X_target, y = df_want.SMILES.values, df_want['Target Sequence'].values, np.array(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### select endcoder ####\n",
    "drug_encoding, target_encoding = 'CNN', 'CNN'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drug Target Interaction Prediction Mode...\n",
      "in total: 1592671 drug-target pairs\n",
      "encoding drug...\n",
      "unique drugs: 770912\n",
      "encoding protein...\n",
      "unique target sequence: 5735\n"
     ]
    }
   ],
   "source": [
    "#### data_process ####\n",
    "\n",
    "# parameter\n",
    "split_method = 'random'\n",
    "frac = [0.7, 0.1, 0.2]\n",
    "random_seed = 1\n",
    "sample_frac = 1\n",
    "mode = 'DTI'\n",
    "X_drug_ = None\n",
    "X_target_ = None\n",
    "\n",
    "# data_process : dataframe\n",
    "\n",
    "if random_seed == 'TDC':\n",
    "\trandom_seed = 1234\n",
    " \n",
    "#property_prediction_flag = X_target is None\n",
    "property_prediction_flag, function_prediction_flag, DDI_flag, PPI_flag, DTI_flag = False, False, False, False, False\n",
    "\n",
    "if (X_drug is not None) and (X_target is not None):\n",
    "\tDTI_flag = True\n",
    "\tif (X_drug is None) or (X_target is None):\n",
    "\t\traise AttributeError(\"Target pair sequence should be in X_target, X_drug\")\n",
    "else:\n",
    "\traise AttributeError(\"Please use the correct mode. Currently, we support DTI, DDI, PPI, Drug Property Prediction and Protein Function Prediction...\")\n",
    "\n",
    "if split_method == 'repurposing_VS':\n",
    "\ty = [-1]*len(X_drug) # create temp y for compatitibility\n",
    "\n",
    "if DTI_flag:\n",
    "\tprint('Drug Target Interaction Prediction Mode...')\n",
    "\tif isinstance(X_target, str):\n",
    "\t\tX_target = [X_target]\n",
    "\tif len(X_target) == 1:\n",
    "\t\t# one target high throughput screening setting\n",
    "\t\tX_target = np.tile(X_target, (length_func(X_drug), ))\n",
    "\n",
    "\tdf_data = pd.DataFrame(zip(X_drug, X_target, y))\n",
    "\tdf_data.rename(columns={0:'SMILES',\n",
    "\t\t\t\t\t\t\t1: 'Target Sequence',\n",
    "\t\t\t\t\t\t\t2: 'Label'}, \n",
    "\t\t\t\t\t\t\tinplace=True)\n",
    "\tprint('in total: ' + str(len(df_data)) + ' drug-target pairs')\n",
    " \n",
    "# data_process : endcoding & splitting\n",
    "\n",
    "if sample_frac != 1:\n",
    "\tdf_data = df_data.sample(frac = sample_frac) #.reset_index(drop = True)\n",
    "\tprint('after subsample: ' + str(len(df_data)) + ' data points...') \n",
    "\n",
    "if DTI_flag:\n",
    "\tdf_data = utils.encode_drug(df_data, drug_encoding)\n",
    "\tdf_data = utils.encode_protein(df_data, target_encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "splitting dataset...\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# dti split\n",
    "if DTI_flag:\n",
    "\tif split_method == 'repurposing_VS':\n",
    "\t\tpass\n",
    "\telse:\n",
    "\t\tprint('splitting dataset...')\n",
    "\n",
    "\tif split_method == 'random': \n",
    "\t\ttrain, val, test = utils.create_fold(df_data, random_seed, frac)\n",
    "\telif split_method == 'cold_drug':\n",
    "\t\ttrain, val, test = utils.create_fold_setting_cold_drug(df_data, random_seed, frac)\n",
    "\telif split_method == 'HTS':\n",
    "\t\ttrain, val, test = utils.create_fold_setting_cold_drug(df_data, random_seed, frac)\n",
    "\t\tval = pd.concat([val[val.Label == 1].drop_duplicates(subset = 'SMILES'), val[val.Label == 0]])\n",
    "\t\ttest = pd.concat([test[test.Label == 1].drop_duplicates(subset = 'SMILES'), test[test.Label == 0]])        \n",
    "\telif split_method == 'cold_protein':\n",
    "\t\ttrain, val, test = utils.create_fold_setting_cold_protein(df_data, random_seed, frac)\n",
    "\telif split_method == 'repurposing_VS':\n",
    "\t\ttrain = df_data\n",
    "\t\tval = df_data\n",
    "\t\ttest = df_data\n",
    "\telif split_method == 'no_split':\n",
    "\t\tprint('do not do train/test split on the data for already splitted data')\n",
    "\t\tresults = df_data.reset_index(drop=True)\n",
    "\telse:\n",
    "\t\traise AttributeError(\"Please select one of the three split method: random, cold_drug, cold_target!\")\n",
    "\n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### save & reset index ####\n",
    "pd.Series(train.index.values, name='index_train').to_csv('C:\\\\Users\\\\user\\\\Desktop\\\\DTI\\\\Database\\\\BindingDB\\\\BindingDB_index_train.tsv', sep='\\t', index=True)\n",
    "pd.Series(val.index.values, name='index_val').to_csv('C:\\\\Users\\\\user\\\\Desktop\\\\DTI\\\\Database\\\\BindingDB\\\\BindingDB_index_val.tsv', sep='\\t', index=True)\n",
    "pd.Series(test.index.values, name='index_test').to_csv('C:\\\\Users\\\\user\\\\Desktop\\\\DTI\\\\Database\\\\BindingDB\\\\BindingDB_index_test.tsv', sep='\\t', index=True)\n",
    "\n",
    "train, val, test = train.reset_index(drop=True), val.reset_index(drop=True), test.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(val.index.values, name='index_val').to_csv('C:\\\\Users\\\\user\\\\Desktop\\\\DTI\\\\Database\\\\BindingDB\\\\BindingDB_index_val.tsv', sep='\\t', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model configuration genreration\n",
    "config = utils.generate_config(drug_encoding = drug_encoding, \n",
    "                         target_encoding = target_encoding, \n",
    "                         cls_hidden_dims = [1024,1024,512], \n",
    "                         train_epoch = 50, \n",
    "                         test_every_X_epoch = 10, \n",
    "                         LR = 0.001, \n",
    "                         batch_size = 256,\n",
    "                         hidden_dim_drug = 128,\n",
    "                         cnn_drug_filters = [32,64,96],\n",
    "                         cnn_drug_kernels = [4,6,8],\n",
    "                         cnn_target_filters = [32,64,96],\n",
    "                         cnn_target_kernels = [4,8,12]\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<DeepPurpose.DTI.DBTA at 0x2799fa84050>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model initialization\n",
    "model = models.model_initialize(**config)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's use 1 GPU!\n",
      "--- Data Preparation ---\n",
      "--- Go for Training ---\n",
      "Training at Epoch 1 iteration 0 with loss 41.8660. Total time 0.00083 hours\n",
      "Training at Epoch 1 iteration 100 with loss 2.08345. Total time 0.02055 hours\n",
      "Training at Epoch 1 iteration 200 with loss 2.37071. Total time 0.04 hours\n",
      "Training at Epoch 1 iteration 300 with loss 1.93328. Total time 0.05888 hours\n",
      "Training at Epoch 1 iteration 400 with loss 1.71777. Total time 0.07833 hours\n",
      "Training at Epoch 1 iteration 500 with loss 1.63799. Total time 0.0975 hours\n",
      "Training at Epoch 1 iteration 600 with loss 1.60238. Total time 0.11666 hours\n",
      "Training at Epoch 1 iteration 700 with loss 1.72801. Total time 0.13611 hours\n",
      "Training at Epoch 1 iteration 800 with loss 1.65600. Total time 0.15527 hours\n",
      "Training at Epoch 1 iteration 900 with loss 1.44310. Total time 0.17444 hours\n",
      "Training at Epoch 1 iteration 1000 with loss 1.45975. Total time 0.19388 hours\n",
      "Training at Epoch 1 iteration 1100 with loss 1.62839. Total time 0.21305 hours\n",
      "Training at Epoch 1 iteration 1200 with loss 1.32658. Total time 0.23222 hours\n",
      "Training at Epoch 1 iteration 1300 with loss 1.50670. Total time 0.25166 hours\n",
      "Training at Epoch 1 iteration 1400 with loss 1.56047. Total time 0.27083 hours\n",
      "Training at Epoch 1 iteration 1500 with loss 1.60779. Total time 0.29027 hours\n",
      "Training at Epoch 1 iteration 1600 with loss 1.35496. Total time 0.30944 hours\n",
      "Training at Epoch 1 iteration 1700 with loss 1.40656. Total time 0.32888 hours\n",
      "Training at Epoch 1 iteration 1800 with loss 1.33469. Total time 0.34833 hours\n",
      "Training at Epoch 1 iteration 1900 with loss 1.55365. Total time 0.36666 hours\n",
      "Training at Epoch 1 iteration 2000 with loss 1.34430. Total time 0.38527 hours\n",
      "Training at Epoch 1 iteration 2100 with loss 1.38362. Total time 0.40361 hours\n",
      "Training at Epoch 1 iteration 2200 with loss 1.54954. Total time 0.42194 hours\n",
      "Training at Epoch 1 iteration 2300 with loss 1.43901. Total time 0.44027 hours\n",
      "Training at Epoch 1 iteration 2400 with loss 1.38262. Total time 0.45861 hours\n",
      "Training at Epoch 1 iteration 2500 with loss 1.23609. Total time 0.47694 hours\n",
      "Training at Epoch 1 iteration 2600 with loss 1.36501. Total time 0.49527 hours\n",
      "Training at Epoch 1 iteration 2700 with loss 1.20627. Total time 0.51361 hours\n",
      "Training at Epoch 1 iteration 2800 with loss 1.25570. Total time 0.53194 hours\n",
      "Training at Epoch 1 iteration 2900 with loss 1.27807. Total time 0.55055 hours\n",
      "Training at Epoch 1 iteration 3000 with loss 1.69057. Total time 0.56888 hours\n",
      "Training at Epoch 1 iteration 3100 with loss 1.22590. Total time 0.58722 hours\n",
      "Training at Epoch 1 iteration 3200 with loss 1.19543. Total time 0.60555 hours\n",
      "Training at Epoch 1 iteration 3300 with loss 1.30494. Total time 0.62388 hours\n",
      "Training at Epoch 1 iteration 3400 with loss 1.41268. Total time 0.64222 hours\n",
      "Training at Epoch 1 iteration 3500 with loss 1.11393. Total time 0.66055 hours\n",
      "Training at Epoch 1 iteration 3600 with loss 1.28453. Total time 0.67888 hours\n",
      "Training at Epoch 1 iteration 3700 with loss 1.16769. Total time 0.69694 hours\n",
      "Training at Epoch 1 iteration 3800 with loss 1.20695. Total time 0.71555 hours\n",
      "Training at Epoch 1 iteration 3900 with loss 1.15822. Total time 0.73388 hours\n",
      "Training at Epoch 1 iteration 4000 with loss 1.35764. Total time 0.75222 hours\n",
      "Training at Epoch 1 iteration 4100 with loss 1.07291. Total time 0.77055 hours\n",
      "Training at Epoch 1 iteration 4200 with loss 1.13539. Total time 0.78888 hours\n",
      "Training at Epoch 1 iteration 4300 with loss 1.41219. Total time 0.8075 hours\n",
      "Validation at Epoch 1 with loss:0.70501, MSE: 1.18652 , Pearson Correlation: 0.68084 with p-value: 0.00E+00 , Concordance Index: 0.74568\n",
      "Training at Epoch 2 iteration 0 with loss 1.25907. Total time 0.91722 hours\n",
      "Training at Epoch 2 iteration 100 with loss 1.08739. Total time 0.93555 hours\n",
      "Training at Epoch 2 iteration 200 with loss 1.20717. Total time 0.95388 hours\n",
      "Training at Epoch 2 iteration 300 with loss 1.27664. Total time 0.97194 hours\n",
      "Training at Epoch 2 iteration 400 with loss 1.20088. Total time 0.99027 hours\n",
      "Training at Epoch 2 iteration 500 with loss 1.77858. Total time 1.00833 hours\n",
      "Training at Epoch 2 iteration 600 with loss 1.23195. Total time 1.02666 hours\n",
      "Training at Epoch 2 iteration 700 with loss 1.47113. Total time 1.04472 hours\n",
      "Training at Epoch 2 iteration 800 with loss 1.21374. Total time 1.06305 hours\n",
      "Training at Epoch 2 iteration 900 with loss 1.03778. Total time 1.08111 hours\n",
      "Training at Epoch 2 iteration 1000 with loss 0.99040. Total time 1.09944 hours\n",
      "Training at Epoch 2 iteration 1100 with loss 1.27814. Total time 1.1175 hours\n",
      "Training at Epoch 2 iteration 1200 with loss 1.11069. Total time 1.13555 hours\n",
      "Training at Epoch 2 iteration 1300 with loss 1.19280. Total time 1.15388 hours\n",
      "Training at Epoch 2 iteration 1400 with loss 1.34280. Total time 1.17194 hours\n",
      "Training at Epoch 2 iteration 1500 with loss 1.06405. Total time 1.19 hours\n",
      "Training at Epoch 2 iteration 1600 with loss 2.04904. Total time 1.20833 hours\n",
      "Training at Epoch 2 iteration 1700 with loss 1.19651. Total time 1.22638 hours\n",
      "Training at Epoch 2 iteration 1800 with loss 1.14582. Total time 1.24444 hours\n",
      "Training at Epoch 2 iteration 1900 with loss 1.25974. Total time 1.26277 hours\n",
      "Training at Epoch 2 iteration 2000 with loss 1.17795. Total time 1.28083 hours\n",
      "Training at Epoch 2 iteration 2100 with loss 1.29120. Total time 1.29888 hours\n",
      "Training at Epoch 2 iteration 2200 with loss 1.17490. Total time 1.31694 hours\n",
      "Training at Epoch 2 iteration 2300 with loss 0.97201. Total time 1.33527 hours\n",
      "Training at Epoch 2 iteration 2400 with loss 1.20930. Total time 1.35333 hours\n",
      "Training at Epoch 2 iteration 2500 with loss 1.04528. Total time 1.37138 hours\n",
      "Training at Epoch 2 iteration 2600 with loss 1.19176. Total time 1.38972 hours\n",
      "Training at Epoch 2 iteration 2700 with loss 0.92371. Total time 1.40777 hours\n",
      "Training at Epoch 2 iteration 2800 with loss 1.20883. Total time 1.42583 hours\n",
      "Training at Epoch 2 iteration 2900 with loss 1.20407. Total time 1.44416 hours\n",
      "Training at Epoch 2 iteration 3000 with loss 1.21655. Total time 1.46222 hours\n",
      "Training at Epoch 2 iteration 3100 with loss 1.44090. Total time 1.48027 hours\n",
      "Training at Epoch 2 iteration 3200 with loss 1.19178. Total time 1.49861 hours\n",
      "Training at Epoch 2 iteration 3300 with loss 1.01549. Total time 1.51666 hours\n",
      "Training at Epoch 2 iteration 3400 with loss 1.37977. Total time 1.53472 hours\n",
      "Training at Epoch 2 iteration 3500 with loss 1.03067. Total time 1.55305 hours\n",
      "Training at Epoch 2 iteration 3600 with loss 1.39089. Total time 1.57111 hours\n",
      "Training at Epoch 2 iteration 3700 with loss 1.11227. Total time 1.58916 hours\n",
      "Training at Epoch 2 iteration 3800 with loss 1.11111. Total time 1.60722 hours\n",
      "Training at Epoch 2 iteration 3900 with loss 1.03728. Total time 1.62555 hours\n",
      "Training at Epoch 2 iteration 4000 with loss 1.10031. Total time 1.64361 hours\n",
      "Training at Epoch 2 iteration 4100 with loss 1.11462. Total time 1.66166 hours\n",
      "Training at Epoch 2 iteration 4200 with loss 1.12205. Total time 1.68 hours\n",
      "Training at Epoch 2 iteration 4300 with loss 1.00150. Total time 1.69805 hours\n",
      "Validation at Epoch 2 with loss:1.90234, MSE: 1.09430 , Pearson Correlation: 0.70931 with p-value: 0.00E+00 , Concordance Index: 0.75888\n",
      "Training at Epoch 3 iteration 0 with loss 1.18682. Total time 1.80722 hours\n",
      "Training at Epoch 3 iteration 100 with loss 1.03792. Total time 1.82555 hours\n",
      "Training at Epoch 3 iteration 200 with loss 1.00043. Total time 1.84388 hours\n",
      "Training at Epoch 3 iteration 300 with loss 1.02535. Total time 1.86222 hours\n",
      "Training at Epoch 3 iteration 400 with loss 1.11613. Total time 1.88027 hours\n",
      "Training at Epoch 3 iteration 500 with loss 0.88493. Total time 1.89861 hours\n",
      "Training at Epoch 3 iteration 600 with loss 1.11137. Total time 1.91666 hours\n",
      "Training at Epoch 3 iteration 700 with loss 1.05987. Total time 1.935 hours\n",
      "Training at Epoch 3 iteration 800 with loss 0.92164. Total time 1.95305 hours\n",
      "Training at Epoch 3 iteration 900 with loss 1.11735. Total time 1.97111 hours\n",
      "Training at Epoch 3 iteration 1000 with loss 1.15131. Total time 1.98944 hours\n",
      "Training at Epoch 3 iteration 1100 with loss 0.88745. Total time 2.0075 hours\n",
      "Training at Epoch 3 iteration 1200 with loss 1.06424. Total time 2.02583 hours\n",
      "Training at Epoch 3 iteration 1300 with loss 1.19219. Total time 2.04388 hours\n",
      "Training at Epoch 3 iteration 1400 with loss 1.11616. Total time 2.06194 hours\n",
      "Training at Epoch 3 iteration 1500 with loss 1.07804. Total time 2.08027 hours\n",
      "Training at Epoch 3 iteration 1600 with loss 1.07282. Total time 2.09833 hours\n",
      "Training at Epoch 3 iteration 1700 with loss 1.10881. Total time 2.11638 hours\n",
      "Training at Epoch 3 iteration 1800 with loss 1.16938. Total time 2.13472 hours\n",
      "Training at Epoch 3 iteration 1900 with loss 1.14413. Total time 2.15277 hours\n",
      "Training at Epoch 3 iteration 2000 with loss 1.02427. Total time 2.17083 hours\n",
      "Training at Epoch 3 iteration 2100 with loss 1.05439. Total time 2.18916 hours\n",
      "Training at Epoch 3 iteration 2200 with loss 1.10150. Total time 2.20722 hours\n",
      "Training at Epoch 3 iteration 2300 with loss 1.15999. Total time 2.22527 hours\n",
      "Training at Epoch 3 iteration 2400 with loss 1.47751. Total time 2.24361 hours\n",
      "Training at Epoch 3 iteration 2500 with loss 1.13664. Total time 2.26166 hours\n",
      "Training at Epoch 3 iteration 2600 with loss 0.95688. Total time 2.27972 hours\n",
      "Training at Epoch 3 iteration 2700 with loss 1.05852. Total time 2.29805 hours\n",
      "Training at Epoch 3 iteration 2800 with loss 1.04886. Total time 2.31611 hours\n",
      "Training at Epoch 3 iteration 2900 with loss 1.22154. Total time 2.33416 hours\n",
      "Training at Epoch 3 iteration 3000 with loss 0.80258. Total time 2.3525 hours\n",
      "Training at Epoch 3 iteration 3100 with loss 1.26507. Total time 2.37055 hours\n",
      "Training at Epoch 3 iteration 3200 with loss 0.99286. Total time 2.38861 hours\n",
      "Training at Epoch 3 iteration 3300 with loss 1.19901. Total time 2.40694 hours\n",
      "Training at Epoch 3 iteration 3400 with loss 1.11801. Total time 2.425 hours\n",
      "Training at Epoch 3 iteration 3500 with loss 1.30650. Total time 2.44305 hours\n",
      "Training at Epoch 3 iteration 3600 with loss 1.23194. Total time 2.46138 hours\n",
      "Training at Epoch 3 iteration 3700 with loss 0.99600. Total time 2.47944 hours\n",
      "Training at Epoch 3 iteration 3800 with loss 1.05722. Total time 2.4975 hours\n",
      "Training at Epoch 3 iteration 3900 with loss 1.11368. Total time 2.51555 hours\n",
      "Training at Epoch 3 iteration 4000 with loss 0.97929. Total time 2.53388 hours\n",
      "Training at Epoch 3 iteration 4100 with loss 0.97498. Total time 2.55194 hours\n",
      "Training at Epoch 3 iteration 4200 with loss 1.07774. Total time 2.57 hours\n",
      "Training at Epoch 3 iteration 4300 with loss 1.03659. Total time 2.58833 hours\n",
      "Validation at Epoch 3 with loss:1.20117, MSE: 1.03126 , Pearson Correlation: 0.73008 with p-value: 0.00E+00 , Concordance Index: 0.76930\n",
      "Training at Epoch 4 iteration 0 with loss 1.04601. Total time 2.69777 hours\n",
      "Training at Epoch 4 iteration 100 with loss 0.85193. Total time 2.71611 hours\n",
      "Training at Epoch 4 iteration 200 with loss 1.06865. Total time 2.73416 hours\n",
      "Training at Epoch 4 iteration 300 with loss 1.15059. Total time 2.7525 hours\n",
      "Training at Epoch 4 iteration 400 with loss 1.04023. Total time 2.77083 hours\n",
      "Training at Epoch 4 iteration 500 with loss 1.72348. Total time 2.78888 hours\n",
      "Training at Epoch 4 iteration 600 with loss 0.95186. Total time 2.80722 hours\n",
      "Training at Epoch 4 iteration 700 with loss 0.94283. Total time 2.82555 hours\n",
      "Training at Epoch 4 iteration 800 with loss 0.95341. Total time 2.84361 hours\n",
      "Training at Epoch 4 iteration 900 with loss 1.16449. Total time 2.86194 hours\n",
      "Training at Epoch 4 iteration 1000 with loss 1.21738. Total time 2.88 hours\n",
      "Training at Epoch 4 iteration 1100 with loss 0.94614. Total time 2.89805 hours\n",
      "Training at Epoch 4 iteration 1200 with loss 0.98357. Total time 2.91638 hours\n",
      "Training at Epoch 4 iteration 1300 with loss 0.90566. Total time 2.93444 hours\n",
      "Training at Epoch 4 iteration 1400 with loss 0.85152. Total time 2.95277 hours\n",
      "Training at Epoch 4 iteration 1500 with loss 1.05516. Total time 2.97111 hours\n",
      "Training at Epoch 4 iteration 1600 with loss 0.90662. Total time 2.98916 hours\n",
      "Training at Epoch 4 iteration 1700 with loss 1.25559. Total time 3.00722 hours\n",
      "Training at Epoch 4 iteration 1800 with loss 0.82815. Total time 3.02555 hours\n",
      "Training at Epoch 4 iteration 1900 with loss 1.03439. Total time 3.04361 hours\n",
      "Training at Epoch 4 iteration 2000 with loss 1.08928. Total time 3.06194 hours\n",
      "Training at Epoch 4 iteration 2100 with loss 0.97744. Total time 3.08 hours\n",
      "Training at Epoch 4 iteration 2200 with loss 0.95066. Total time 3.09805 hours\n",
      "Training at Epoch 4 iteration 2300 with loss 0.85500. Total time 3.11638 hours\n",
      "Training at Epoch 4 iteration 2400 with loss 1.00422. Total time 3.13444 hours\n",
      "Training at Epoch 4 iteration 2500 with loss 1.01306. Total time 3.15277 hours\n",
      "Training at Epoch 4 iteration 2600 with loss 0.87285. Total time 3.17083 hours\n",
      "Training at Epoch 4 iteration 2700 with loss 1.02496. Total time 3.18888 hours\n",
      "Training at Epoch 4 iteration 2800 with loss 1.05801. Total time 3.20722 hours\n",
      "Training at Epoch 4 iteration 2900 with loss 0.87073. Total time 3.22527 hours\n",
      "Training at Epoch 4 iteration 3000 with loss 1.01815. Total time 3.24361 hours\n",
      "Training at Epoch 4 iteration 3100 with loss 1.00194. Total time 3.26166 hours\n",
      "Training at Epoch 4 iteration 3200 with loss 1.07043. Total time 3.28027 hours\n",
      "Training at Epoch 4 iteration 3300 with loss 1.02048. Total time 3.29944 hours\n",
      "Training at Epoch 4 iteration 3400 with loss 1.05039. Total time 3.31888 hours\n",
      "Training at Epoch 4 iteration 3500 with loss 0.93054. Total time 3.33805 hours\n",
      "Training at Epoch 4 iteration 3600 with loss 0.90599. Total time 3.3575 hours\n",
      "Training at Epoch 4 iteration 3700 with loss 1.02885. Total time 3.37666 hours\n",
      "Training at Epoch 4 iteration 3800 with loss 0.91515. Total time 3.39611 hours\n",
      "Training at Epoch 4 iteration 3900 with loss 0.97173. Total time 3.41527 hours\n",
      "Training at Epoch 4 iteration 4000 with loss 1.11203. Total time 3.43444 hours\n",
      "Training at Epoch 4 iteration 4100 with loss 1.09241. Total time 3.45361 hours\n",
      "Training at Epoch 4 iteration 4200 with loss 1.05208. Total time 3.47277 hours\n",
      "Training at Epoch 4 iteration 4300 with loss 1.08286. Total time 3.49166 hours\n",
      "Validation at Epoch 4 with loss:1.12585, MSE: 0.98503 , Pearson Correlation: 0.74348 with p-value: 0.00E+00 , Concordance Index: 0.77585\n",
      "Training at Epoch 5 iteration 0 with loss 0.85874. Total time 3.60333 hours\n",
      "Training at Epoch 5 iteration 100 with loss 0.95181. Total time 3.6225 hours\n",
      "Training at Epoch 5 iteration 200 with loss 0.97039. Total time 3.64166 hours\n",
      "Training at Epoch 5 iteration 300 with loss 0.89976. Total time 3.66083 hours\n",
      "Training at Epoch 5 iteration 400 with loss 1.08308. Total time 3.68 hours\n",
      "Training at Epoch 5 iteration 500 with loss 0.99683. Total time 3.69916 hours\n",
      "Training at Epoch 5 iteration 600 with loss 0.95046. Total time 3.71833 hours\n",
      "Training at Epoch 5 iteration 700 with loss 1.17960. Total time 3.7375 hours\n",
      "Training at Epoch 5 iteration 800 with loss 1.07167. Total time 3.75666 hours\n",
      "Training at Epoch 5 iteration 900 with loss 1.01254. Total time 3.77583 hours\n",
      "Training at Epoch 5 iteration 1000 with loss 0.99748. Total time 3.79527 hours\n",
      "Training at Epoch 5 iteration 1100 with loss 0.85359. Total time 3.81444 hours\n",
      "Training at Epoch 5 iteration 1200 with loss 1.14316. Total time 3.83388 hours\n",
      "Training at Epoch 5 iteration 1300 with loss 0.95992. Total time 3.85305 hours\n",
      "Training at Epoch 5 iteration 1400 with loss 0.96806. Total time 3.87222 hours\n",
      "Training at Epoch 5 iteration 1500 with loss 1.16194. Total time 3.89138 hours\n",
      "Training at Epoch 5 iteration 1600 with loss 0.90851. Total time 3.91083 hours\n",
      "Training at Epoch 5 iteration 1700 with loss 1.15380. Total time 3.93 hours\n",
      "Training at Epoch 5 iteration 1800 with loss 1.06920. Total time 3.94944 hours\n",
      "Training at Epoch 5 iteration 1900 with loss 1.55880. Total time 3.96861 hours\n",
      "Training at Epoch 5 iteration 2000 with loss 0.96602. Total time 3.98777 hours\n",
      "Training at Epoch 5 iteration 2100 with loss 1.19494. Total time 4.00722 hours\n",
      "Training at Epoch 5 iteration 2200 with loss 0.99874. Total time 4.02638 hours\n",
      "Training at Epoch 5 iteration 2300 with loss 1.02939. Total time 4.04555 hours\n",
      "Training at Epoch 5 iteration 2400 with loss 1.01282. Total time 4.065 hours\n",
      "Training at Epoch 5 iteration 2500 with loss 0.90446. Total time 4.08416 hours\n",
      "Training at Epoch 5 iteration 2600 with loss 1.01213. Total time 4.10333 hours\n",
      "Training at Epoch 5 iteration 2700 with loss 1.05051. Total time 4.12277 hours\n",
      "Training at Epoch 5 iteration 2800 with loss 0.97631. Total time 4.14194 hours\n",
      "Training at Epoch 5 iteration 2900 with loss 0.79304. Total time 4.16111 hours\n",
      "Training at Epoch 5 iteration 3000 with loss 0.83175. Total time 4.18055 hours\n",
      "Training at Epoch 5 iteration 3100 with loss 0.86621. Total time 4.19972 hours\n",
      "Training at Epoch 5 iteration 3200 with loss 0.84363. Total time 4.21888 hours\n",
      "Training at Epoch 5 iteration 3300 with loss 0.83280. Total time 4.23805 hours\n",
      "Training at Epoch 5 iteration 3400 with loss 0.90013. Total time 4.25722 hours\n",
      "Training at Epoch 5 iteration 3500 with loss 1.08690. Total time 4.27638 hours\n",
      "Training at Epoch 5 iteration 3600 with loss 0.96022. Total time 4.29555 hours\n",
      "Training at Epoch 5 iteration 3700 with loss 1.07952. Total time 4.31472 hours\n",
      "Training at Epoch 5 iteration 3800 with loss 1.06547. Total time 4.33361 hours\n",
      "Training at Epoch 5 iteration 3900 with loss 0.99443. Total time 4.35222 hours\n",
      "Training at Epoch 5 iteration 4000 with loss 0.84285. Total time 4.37055 hours\n",
      "Training at Epoch 5 iteration 4100 with loss 0.92485. Total time 4.38888 hours\n",
      "Training at Epoch 5 iteration 4200 with loss 0.90465. Total time 4.4075 hours\n",
      "Training at Epoch 5 iteration 4300 with loss 0.93855. Total time 4.42583 hours\n",
      "Validation at Epoch 5 with loss:1.67704, MSE: 0.96179 , Pearson Correlation: 0.75729 with p-value: 0.00E+00 , Concordance Index: 0.78268\n",
      "Training at Epoch 6 iteration 0 with loss 0.84702. Total time 4.53611 hours\n",
      "Training at Epoch 6 iteration 100 with loss 0.98132. Total time 4.55527 hours\n",
      "Training at Epoch 6 iteration 200 with loss 1.07677. Total time 4.57444 hours\n",
      "Training at Epoch 6 iteration 300 with loss 1.09953. Total time 4.59277 hours\n",
      "Training at Epoch 6 iteration 400 with loss 1.17374. Total time 4.61111 hours\n",
      "Training at Epoch 6 iteration 500 with loss 0.93092. Total time 4.62944 hours\n",
      "Training at Epoch 6 iteration 600 with loss 0.91811. Total time 4.64777 hours\n",
      "Training at Epoch 6 iteration 700 with loss 1.11052. Total time 4.66611 hours\n",
      "Training at Epoch 6 iteration 800 with loss 1.06558. Total time 4.68444 hours\n",
      "Training at Epoch 6 iteration 900 with loss 0.94854. Total time 4.70277 hours\n",
      "Training at Epoch 6 iteration 1000 with loss 0.89210. Total time 4.72222 hours\n",
      "Training at Epoch 6 iteration 1100 with loss 1.01509. Total time 4.74166 hours\n",
      "Training at Epoch 6 iteration 1200 with loss 0.89622. Total time 4.76111 hours\n",
      "Training at Epoch 6 iteration 1300 with loss 0.93065. Total time 4.78027 hours\n",
      "Training at Epoch 6 iteration 1400 with loss 0.95107. Total time 4.79944 hours\n",
      "Training at Epoch 6 iteration 1500 with loss 1.08219. Total time 4.81861 hours\n",
      "Training at Epoch 6 iteration 1600 with loss 1.03879. Total time 4.83777 hours\n",
      "Training at Epoch 6 iteration 1700 with loss 0.84025. Total time 4.85694 hours\n",
      "Training at Epoch 6 iteration 1800 with loss 0.93205. Total time 4.87611 hours\n",
      "Training at Epoch 6 iteration 1900 with loss 0.86596. Total time 4.89527 hours\n",
      "Training at Epoch 6 iteration 2000 with loss 0.72009. Total time 4.91472 hours\n",
      "Training at Epoch 6 iteration 2100 with loss 0.87440. Total time 4.93388 hours\n",
      "Training at Epoch 6 iteration 2200 with loss 0.87962. Total time 4.95333 hours\n",
      "Training at Epoch 6 iteration 2300 with loss 1.10130. Total time 4.97222 hours\n",
      "Training at Epoch 6 iteration 2400 with loss 1.02429. Total time 4.99083 hours\n",
      "Training at Epoch 6 iteration 2500 with loss 1.03533. Total time 5.01 hours\n",
      "Training at Epoch 6 iteration 2600 with loss 1.02303. Total time 5.02944 hours\n",
      "Training at Epoch 6 iteration 2700 with loss 1.07508. Total time 5.04861 hours\n",
      "Training at Epoch 6 iteration 2800 with loss 0.87960. Total time 5.06805 hours\n",
      "Training at Epoch 6 iteration 2900 with loss 1.02609. Total time 5.08722 hours\n",
      "Training at Epoch 6 iteration 3000 with loss 1.02643. Total time 5.10666 hours\n",
      "Training at Epoch 6 iteration 3100 with loss 1.06601. Total time 5.12583 hours\n",
      "Training at Epoch 6 iteration 3200 with loss 0.82794. Total time 5.145 hours\n",
      "Training at Epoch 6 iteration 3300 with loss 0.85610. Total time 5.16416 hours\n",
      "Training at Epoch 6 iteration 3400 with loss 0.97622. Total time 5.18305 hours\n",
      "Training at Epoch 6 iteration 3500 with loss 0.81827. Total time 5.20166 hours\n",
      "Training at Epoch 6 iteration 3600 with loss 1.06950. Total time 5.22055 hours\n",
      "Training at Epoch 6 iteration 3700 with loss 0.95738. Total time 5.23944 hours\n",
      "Training at Epoch 6 iteration 3800 with loss 0.86817. Total time 5.25888 hours\n",
      "Training at Epoch 6 iteration 3900 with loss 0.87239. Total time 5.27833 hours\n",
      "Training at Epoch 6 iteration 4000 with loss 0.88497. Total time 5.2975 hours\n",
      "Training at Epoch 6 iteration 4100 with loss 1.13554. Total time 5.31694 hours\n",
      "Training at Epoch 6 iteration 4200 with loss 1.03131. Total time 5.33611 hours\n",
      "Training at Epoch 6 iteration 4300 with loss 0.85152. Total time 5.35527 hours\n",
      "Validation at Epoch 6 with loss:1.08131, MSE: 0.97414 , Pearson Correlation: 0.76287 with p-value: 0.00E+00 , Concordance Index: 0.78601\n",
      "Training at Epoch 7 iteration 0 with loss 0.86832. Total time 5.46638 hours\n",
      "Training at Epoch 7 iteration 100 with loss 0.96183. Total time 5.48527 hours\n",
      "Training at Epoch 7 iteration 200 with loss 0.87581. Total time 5.50388 hours\n",
      "Training at Epoch 7 iteration 300 with loss 0.99351. Total time 5.5225 hours\n",
      "Training at Epoch 7 iteration 400 with loss 0.87930. Total time 5.54138 hours\n",
      "Training at Epoch 7 iteration 500 with loss 0.78912. Total time 5.56 hours\n",
      "Training at Epoch 7 iteration 600 with loss 0.86337. Total time 5.57861 hours\n",
      "Training at Epoch 7 iteration 700 with loss 1.04756. Total time 5.59805 hours\n",
      "Training at Epoch 7 iteration 800 with loss 0.98950. Total time 5.61722 hours\n",
      "Training at Epoch 7 iteration 900 with loss 0.99631. Total time 5.63666 hours\n",
      "Training at Epoch 7 iteration 1000 with loss 0.92715. Total time 5.65611 hours\n",
      "Training at Epoch 7 iteration 1100 with loss 0.80641. Total time 5.67527 hours\n",
      "Training at Epoch 7 iteration 1200 with loss 0.92832. Total time 5.69444 hours\n",
      "Training at Epoch 7 iteration 1300 with loss 0.99089. Total time 5.71388 hours\n",
      "Training at Epoch 7 iteration 1400 with loss 0.98127. Total time 5.73305 hours\n",
      "Training at Epoch 7 iteration 1500 with loss 1.06226. Total time 5.7525 hours\n",
      "Training at Epoch 7 iteration 1600 with loss 0.81297. Total time 5.77166 hours\n",
      "Training at Epoch 7 iteration 1700 with loss 0.90507. Total time 5.79111 hours\n",
      "Training at Epoch 7 iteration 1800 with loss 0.92420. Total time 5.81083 hours\n",
      "Training at Epoch 7 iteration 1900 with loss 0.97555. Total time 5.83027 hours\n",
      "Training at Epoch 7 iteration 2000 with loss 0.91324. Total time 5.84972 hours\n",
      "Training at Epoch 7 iteration 2100 with loss 0.98818. Total time 5.86916 hours\n",
      "Training at Epoch 7 iteration 2200 with loss 1.00661. Total time 5.88833 hours\n",
      "Training at Epoch 7 iteration 2300 with loss 0.96231. Total time 5.9075 hours\n",
      "Training at Epoch 7 iteration 2400 with loss 0.92176. Total time 5.92694 hours\n",
      "Training at Epoch 7 iteration 2500 with loss 1.08292. Total time 5.94611 hours\n",
      "Training at Epoch 7 iteration 2600 with loss 0.95458. Total time 5.96555 hours\n",
      "Training at Epoch 7 iteration 2700 with loss 0.91821. Total time 5.98472 hours\n",
      "Training at Epoch 7 iteration 2800 with loss 0.93055. Total time 6.00416 hours\n",
      "Training at Epoch 7 iteration 2900 with loss 0.86455. Total time 6.02333 hours\n",
      "Training at Epoch 7 iteration 3000 with loss 0.89210. Total time 6.04166 hours\n",
      "Training at Epoch 7 iteration 3100 with loss 0.92689. Total time 6.05972 hours\n",
      "Training at Epoch 7 iteration 3200 with loss 1.23159. Total time 6.07805 hours\n",
      "Training at Epoch 7 iteration 3300 with loss 1.08220. Total time 6.09666 hours\n",
      "Training at Epoch 7 iteration 3400 with loss 0.84557. Total time 6.11555 hours\n",
      "Training at Epoch 7 iteration 3500 with loss 0.96671. Total time 6.13388 hours\n",
      "Training at Epoch 7 iteration 3600 with loss 0.92557. Total time 6.1525 hours\n",
      "Training at Epoch 7 iteration 3700 with loss 0.90059. Total time 6.17138 hours\n",
      "Training at Epoch 7 iteration 3800 with loss 0.62971. Total time 6.19 hours\n",
      "Training at Epoch 7 iteration 3900 with loss 0.79724. Total time 6.20861 hours\n",
      "Training at Epoch 7 iteration 4000 with loss 0.73800. Total time 6.22694 hours\n",
      "Training at Epoch 7 iteration 4100 with loss 0.92662. Total time 6.24555 hours\n",
      "Training at Epoch 7 iteration 4200 with loss 0.80130. Total time 6.26444 hours\n",
      "Training at Epoch 7 iteration 4300 with loss 0.86171. Total time 6.28305 hours\n",
      "Validation at Epoch 7 with loss:0.86258, MSE: 0.91015 , Pearson Correlation: 0.76986 with p-value: 0.00E+00 , Concordance Index: 0.78916\n",
      "Training at Epoch 8 iteration 0 with loss 1.01074. Total time 6.39472 hours\n",
      "Training at Epoch 8 iteration 100 with loss 0.78361. Total time 6.41361 hours\n",
      "Training at Epoch 8 iteration 200 with loss 0.73630. Total time 6.43194 hours\n",
      "Training at Epoch 8 iteration 300 with loss 0.82709. Total time 6.45055 hours\n",
      "Training at Epoch 8 iteration 400 with loss 0.84843. Total time 6.46888 hours\n",
      "Training at Epoch 8 iteration 500 with loss 0.97299. Total time 6.48722 hours\n",
      "Training at Epoch 8 iteration 600 with loss 0.87965. Total time 6.50583 hours\n",
      "Training at Epoch 8 iteration 700 with loss 0.80442. Total time 6.52444 hours\n",
      "Training at Epoch 8 iteration 800 with loss 0.91397. Total time 6.54277 hours\n",
      "Training at Epoch 8 iteration 900 with loss 0.74004. Total time 6.56083 hours\n",
      "Training at Epoch 8 iteration 1000 with loss 0.89795. Total time 6.57916 hours\n",
      "Training at Epoch 8 iteration 1100 with loss 0.98798. Total time 6.59722 hours\n",
      "Training at Epoch 8 iteration 1200 with loss 0.86292. Total time 6.61555 hours\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Model training\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Model saving\u001b[39;00m\n\u001b[0;32m      5\u001b[0m model\u001b[38;5;241m.\u001b[39msave_model(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./pretrained_model/CNN_CNN_KIBA_after/\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\user\\Desktop\\DTI\\DeepPurpose-1\\DeepPurpose\\DTI.py:437\u001b[0m, in \u001b[0;36mDBTA.train\u001b[1;34m(self, train, val, test, verbose)\u001b[0m\n\u001b[0;32m    434\u001b[0m \t\u001b[38;5;66;03m#score = self.model(v_d, v_p.float().to(self.device))\u001b[39;00m\n\u001b[0;32m    436\u001b[0m score \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(v_d, v_p)\n\u001b[1;32m--> 437\u001b[0m label \u001b[38;5;241m=\u001b[39m \u001b[43mVariable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    439\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbinary:\n\u001b[0;32m    440\u001b[0m \tloss_fct \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mBCELoss()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Model training\n",
    "model.train(train, val, test)\n",
    "\n",
    "# Model saving\n",
    "model.save_model('./pretrained_model/CNN_CNN_KIBA_after/')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py_dp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
